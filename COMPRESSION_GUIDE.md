# Model Compression Toolkit - QTinker Enhancement

## Overview

QTinker now includes a comprehensive model compression suite with support for:
- **Quantization**: TorchAO, GPTQ, AWQ, ONNX, Bitsandbytes
- **Pruning**: Magnitude, Structured, Global, SparseML
- **Distillation**: Knowledge transfer, Logit KD, Feature KD
- **Export**: ONNX, GGUF, TensorFlow Lite, OpenVINO
- **Pipeline**: End-to-end compression workflows

## Installation

The compression toolkit is automatically installed during `install.js` setup. All required libraries:

```bash
# Core compression frameworks
pip install torchao>=0.2.0
pip install auto-gptq>=0.7.0
pip install autoawq>=0.2.0
pip install sparseml>=1.7.0

# Distillation and model optimization
pip install sentence-transformers>=2.7.0
pip install optimum[onnx,exporters]>=1.13.0

# Cross-platform optimization
pip install neural-compressor>=3.1.0
pip install openvino>=2024.1.0
pip install neural-speed>=2.0.0

# Export and conversion
pip install onnx-simplifier>=0.4.33
pip install llama-cpp-python>=0.2.0
```

## Quick Start

### 1. Using the Web UI (Gradio)

Launch QTinker and navigate to the compression tabs:

```bash
# In Pinokio, click "Start" to launch the web UI
# Compression features are available in separate tabs:
# - üî¢ Quantization
# - ‚úÇÔ∏è Pruning  
# - üßë‚Äçüéì Distillation
# - üîó Pipeline
# - üìä Comparison
```

### 2. Using Python API (Programmatic)

```python
from compression_toolkit import QuantizationToolkit, PruningToolkit, CompressionPipeline
import torch.nn as nn

# Load your model
model = load_your_model()

# Option A: Individual compression
compressed = QuantizationToolkit.quantize_with_torchao(model, method="int8")
compressed = PruningToolkit.magnitude_pruning(compressed, amount=0.3)

# Option B: Full pipeline
pipeline = CompressionPipeline(model, output_dir="compressed_models")
result = pipeline.full_compression(
    prune_amount=0.3,
    quantize_method="int8",
    export_format="onnx"
)
```

## Compression Methods

### Quantization

#### TorchAO (Native PyTorch)
- **Best For**: Research, PyTorch-native workflows
- **Supports**: INT4, INT8, FP8, NF4
- **Pros**: Native integration, no conversion needed
- **Cons**: Limited hardware support

```python
from compression_toolkit import QuantizationToolkit

model = QuantizationToolkit.quantize_with_torchao(
    model, 
    method="int8"  # or "int4", "fp8", "nf4"
)
```

#### GPTQ (Post-Training Quantization)
- **Best For**: Large language models (Llama, Mistral, Falcon)
- **Supports**: 4-bit, 8-bit post-training
- **Pros**: No retraining, fast inference
- **Cons**: Slightly lower accuracy than QAT

```python
QuantizationToolkit.quantize_with_gptq(
    model_name="meta-llama/Llama-2-7b",
    output_path="./quantized_llm",
    bits=4,
    group_size=128
)
```

#### AWQ (Activation-Aware)
- **Best For**: LLMs with accuracy requirements
- **Supports**: 4-bit quantization
- **Pros**: Better accuracy than GPTQ
- **Cons**: Slower than GPTQ

```python
QuantizationToolkit.quantize_with_awq(
    model_name="meta-llama/Llama-2-7b",
    output_path="./quantized_llm",
    bits=4
)
```

#### ONNX Runtime
- **Best For**: Cross-platform deployment
- **Supports**: INT8, FP16
- **Pros**: Hardware-specific optimization
- **Cons**: Requires ONNX export

```python
QuantizationToolkit.quantize_with_onnx(
    model_path="./model.onnx",
    output_path="./quantized_onnx",
    quant_type="QInt8"
)
```

### Pruning

#### Magnitude Pruning
- **Strategy**: Removes individual weights with smallest magnitude
- **Type**: Unstructured (fine-grained)
- **Best For**: Compression ratio, flexibility
- **Accuracy**: High retention (95-99%)

```python
from compression_toolkit import PruningToolkit

model = PruningToolkit.magnitude_pruning(
    model,
    amount=0.3,  # Remove 30% of weights
    layer_types=(nn.Conv2d, nn.Linear)
)
```

#### Structured Pruning
- **Strategy**: Removes entire channels/filters
- **Type**: Structured (coarse-grained)
- **Best For**: Hardware acceleration, mobile deployment
- **Accuracy**: Medium retention (94-97%)

```python
model = PruningToolkit.structured_pruning(
    model,
    amount=0.2,  # Remove 20% of channels
    layer_types=(nn.Conv2d,)
)
```

#### Global Pruning
- **Strategy**: Removes lowest-importance weights across entire model
- **Type**: Unstructured but globally aware
- **Best For**: Optimal layer-wise compression
- **Accuracy**: High retention (96-99%)

```python
model = PruningToolkit.global_pruning(
    model,
    amount=0.3  # 30% global sparsity
)
```

#### SparseML
- **Strategy**: Recipe-based pruning with gradual sparsification
- **Type**: Configurable (structured/unstructured)
- **Best For**: Production-grade compression with fine control
- **Accuracy**: Maximum retention (98-99%)

```python
PruningToolkit.sparseml_pruning(
    model_path="./model.pt",
    output_path="./pruned_model",
    recipe_path="./pruning_recipe.yaml"
)
```

### Distillation

#### Knowledge Distillation
- **Teacher**: Large, accurate model
- **Student**: Small, efficient model
- **Loss**: Combination of soft targets (KL divergence) + hard targets (CE loss)

```python
from compression_toolkit import DistillationToolkit

# Create distillation loss
distill_loss = DistillationToolkit.create_distillation_loss(
    temperature=4.0,  # Controls softness of targets
    alpha=0.7         # Balance soft/hard loss
)

# During training:
# soft_loss = KL(student || teacher) * T^2
# hard_loss = CE(student, true_labels)
# total_loss = alpha * soft_loss + (1-alpha) * hard_loss
```

#### Transformer Distillation
- **Best For**: BERT, RoBERTa, T5 models
- **Framework**: Hugging Face Transformers

```python
DistillationToolkit.distill_with_transformers(
    teacher_model_name="bert-base-uncased",
    student_model_name="distilbert-base-uncased",
    train_dataset=your_dataset,
    output_path="./distilled_model",
    num_epochs=3,
    temperature=4.0
)
```

### Export Formats

#### ONNX (Open Neural Network Exchange)
- **Use Case**: Cross-platform deployment
- **Hardware**: CPU, GPU, mobile
- **Framework**: Framework-agnostic

```python
from compression_toolkit import ExportToolkit

ExportToolkit.export_to_onnx(
    model=your_model,
    output_path="./model.onnx",
    sample_input=torch.randn(1, 3, 224, 224),
    input_names=['image'],
    output_names=['prediction']
)
```

#### GGUF (Quantized LLM Format)
- **Use Case**: Local LLM inference via llama.cpp
- **Hardware**: CPU (with GPU acceleration options)
- **Framework**: Optimized for Llama-family models

```python
ExportToolkit.export_to_gguf(
    model_name="meta-llama/Llama-2-7b",
    output_path="./model.gguf",
    quantization_type="q4_0"  # 4-bit quantization
)
```

#### OpenVINO IR
- **Use Case**: Intel hardware optimization
- **Hardware**: Intel CPU, iGPU, Movidius VPU
- **Framework**: Cross-platform IR format

```python
ExportToolkit.export_to_openvino(
    model_path="./pytorch_model.pt",
    output_path="./openvino_model",
    framework="pytorch"
)
```

## Compression Presets

Use pre-configured presets for common scenarios:

### Light (10-20% compression)
```yaml
pruning: 10%
quantization: int8
export: onnx
expected_accuracy: 99%+
use_case: Demos, development
```

### Medium (40-60% compression)
```yaml
pruning: 30%
quantization: int8
export: onnx
expected_accuracy: 97-98%
use_case: Production deployment
```

### Aggressive (75-90% compression)
```yaml
pruning: 50%
quantization: int4
export: gguf
expected_accuracy: 94-96%
use_case: Mobile/Edge devices
```

### LLM GPTQ (4-bit for LLMs)
```yaml
quantization: gptq (4-bit)
no_pruning: true
export: gguf
expected_accuracy: 99%+
size_reduction: 75% (7B ‚Üí 2.5GB)
use_case: Llama-2, Mistral, Qwen
```

## End-to-End Pipeline

The `CompressionPipeline` class combines multiple techniques:

```python
from compression_toolkit import CompressionPipeline

pipeline = CompressionPipeline(model, output_dir="compressed_models")

# Combined: Prune ‚Üí Quantize ‚Üí Export
result = pipeline.full_compression(
    prune_amount=0.3,        # 30% magnitude pruning
    quantize_method="int8",  # INT8 quantization
    export_format="onnx"     # ONNX export
)

# Result contains:
# - model: Compressed model
# - output_dir: Save location
# - compression_config: Applied configuration
```

## Performance Metrics

### Compression Ratios
| Method | Size Reduction | Accuracy Drop | Latency Improvement |
|--------|---|---|---|
| Magnitude Pruning (30%) | 20% | <1% | 0% (dense ops) |
| Structured Pruning (30%) | 30% | 1-2% | 15-30% |
| INT8 Quantization | 75% | 0-1% | 2-4x |
| INT4 Quantization (GPTQ) | 75% | <1% | 2-4x |
| Pruning (30%) + INT8 | 80% | 1-2% | 2-3x |
| Distillation | 40-60% | 1-3% | 1-2x |
| Full Pipeline (Prune+Quant) | 85% | 1-3% | 2-4x |

### Hardware-Specific Optimization
- **NVIDIA GPUs**: TensorRT, CUDA kernels
- **Intel CPUs**: OpenVINO, Neural Compressor
- **Apple Silicon**: CoreML, MPS acceleration
- **Mobile**: TFLite, ONNX Runtime, llama.cpp
- **Edge Devices**: OpenVINO, Neural Speed, GGUF

## Configuration

Edit `compression_config.yaml` to customize:
- Pruning presets
- Quantization methods
- Export formats
- Hardware profiles
- Quality vs Speed trade-offs
- Common use case recommendations

## Troubleshooting

### Out of Memory
```python
# Use smaller batch sizes
# Enable gradient checkpointing
# Use dynamic quantization instead of static
```

### Low Accuracy After Compression
```python
# Reduce compression amount
# Use distillation instead of pruning alone
# Fine-tune after quantization
# Use QAT (Quantization-Aware Training) instead of PTQ
```

### Slow Inference After Compression
```python
# Use structured pruning (hardware-friendly)
# Increase quantization precision
# Check for dense matrix operations in compressed model
# Profile with hardware-specific tools
```

## Integration with Existing Features

The compression toolkit integrates seamlessly with QTinker's existing features:
- **Model Loader**: Load any HuggingFace or local model
- **Gradio UI**: All compression methods accessible via web interface
- **Device Manager**: Automatic GPU/CPU detection
- **File Browser**: Save/load compressed models
- **Model Registry**: Track compression history

## Advanced Usage

### Custom Compression Recipes
```python
# Create custom compression pipeline
from compression_toolkit import CompressionPipeline, PruningToolkit, QuantizationToolkit

pipeline = CompressionPipeline(model)

# Step 1: Iterative pruning with retraining
for round in range(3):
    model = PruningToolkit.magnitude_pruning(model, amount=0.1)
    # Fine-tune model...

# Step 2: Quantization-aware training
model = prepare_qat(model)
# Train with QAT...

# Step 3: Export to multiple formats
pipeline.export_to_multiple_formats(['onnx', 'gguf', 'openvino'])
```

### Performance Profiling
```python
from compression_toolkit import calculate_model_size, compare_models

original_size = calculate_model_size(original_model)
compressed_size = calculate_model_size(compressed_model)
comparison = compare_models(original_model, compressed_model)

print(f"Compression ratio: {comparison['reduction_percent']:.1f}%")
print(f"Size reduction: {comparison['size_reduction_mb']:.2f} MB")
```

## Resources

- **TorchAO Docs**: https://github.com/pytorch-labs/ao
- **GPTQ Paper**: https://arxiv.org/abs/2210.17323
- **AWQ Paper**: https://arxiv.org/abs/2306.00978
- **SparseML Recipes**: https://github.com/neuralmagic/sparseml
- **Hugging Face Optimum**: https://github.com/huggingface/optimum
- **OpenVINO**: https://github.com/openvinotoolkit/openvino
- **Neural Compressor**: https://github.com/intel/neural-compressor

## Support & Issues

For issues or feature requests related to compression:
1. Check `logs/api/` for detailed error logs
2. Review compression_config.yaml for preset validation
3. Ensure all dependencies are installed: `uv pip list | grep -E "torchao|gptq|awq|sparseml"`
4. Check hardware compatibility with selected compression method

---

**QTinker Compression Toolkit v1.0** - Built for production model optimization
