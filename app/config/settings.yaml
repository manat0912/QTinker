distillation:
  alpha: 0.5
  batch_size: 4
  enabled: true
  epochs: 1
  learning_rate: 5e-5
  max_steps: 100
  mode: placeholder
  teacher_arch: Masked LM (BERT, RoBERTa)
  teacher_model_path: C:\pinokio\api\QTinker\app\bert_models\bert_large\bert-large-uncased-wwm
  teacher_type: Weights & Biases
  temperature: 1.0
local_llm:
  api_key: ''
  base_url: http://localhost:1234/v1
  enabled: false
  model_name: ''
  ollama_url: http://localhost:11434
  provider: lm_studio
ui:
  allow_raw_pt: true
  theme: soft
  title: Distill & Quantize App
